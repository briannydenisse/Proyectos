# -*- coding: utf-8 -*-
"""Proyecto Final - Brianny Hernandez.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14uWeCjtAKF2Ns7RBl9dCPh46cBiAbx-h

### Importamos las librerias que vamos a utilizar
"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer
import warnings
# Ignorar todos los mensajes de advertencia
warnings.filterwarnings("ignore")

"""### Importamos el dataset para el practico"""

########################################################
##INICIO DE LA PARTE A COMPLETAR
########################################################

#Aplique la ruta correcta utilizando la ubicacion en donde alojo el archivo "data.csv"
from google.colab import files
uploaded = files.upload()

########################################################
##FIN DE LA PARTE A COMPLETAR
########################################################

# Leer el archivo CSV en un DataFrame
df = pd.read_csv("data.csv")

print(df.shape)

# Configurar pandas para mostrar todas las columnas
pd.set_option('display.max_columns', None)

# Mostrar el conjunto de datos
df.sample(5)

"""#### A continuacion graficaremos los datos, para evaluar cuantos tumores son benignos y malignos, la idea de este practico, es evaluar si aplicando un algoritmo de CLUSTERING, podemos encontrar 2 agrupaciones o mas, que nos permitan identificar si los tumores son benignos o malignos, de acuerdo al cluster que pertenezcan."""

# Crear un gráfico de conteo para la variable "diagnosis"
plt.figure(figsize=(6, 4))
ax = sns.countplot(x='diagnosis', data=df, palette='Set1')
ax.set(xlabel='Diagnosis', ylabel='Count', title='Count of Benign and Malignant Tumors')
plt.show()

# Obtener y mostrar el número de casos benignos (B) y malignos (M)
B, M = df['diagnosis'].value_counts()
print('Number of Benign (B): ', B)
print('Number of Malignant (M): ', M)

"""#### Como se observa la mayoria de las muestras, son tumores Benignos (357 casos) y (212 casos) son malignos, lo cual demuestra que es un dataset balanceado.

##### Remplazamos el TARGET B y M, por 0 Y 1 respectivamente, para mas adelante realizar una Confusion Matrix y evaluar los resultados, de forma simple. A continuacion eliminamos las 2 columnas ID ya que no es un campo relevante para el algoritmo y el campo diagnosis que es el TARGET, o campo que utlizaremos para contrastar resultados, el foco de este practico es confirmar si los clusteres detectados permiten separar los 2 tipos de tumores.
"""

# Convertir las etiquetas de "diagnosis" a números (0 y 1) para mas adelante hacer una Confusion Matrix
df['diagnosis'] = df['diagnosis'].map({'B': 0, 'M': 1})

# Eliminar las columnas "diagnosis" e "id" del conjunto de características, solo dejamos las variables sin ningun target que permita identificar de antemano que tipo de tumor es
X = df.drop(['diagnosis', 'id'], axis=1)

X.sample(5)

"""#### Estandarizamos las caracteristicas, para mas adelante aplicar K-Means y que todas esten expresadas en la misma escala

"""

# Estandarizar las características para el K-Means
#Escalar X y generar el vector llamado X_scaled
scaler = StandardScaler()





########################################################
##INICIO DE LA PARTE A COMPLETAR
########################################################

#Aplicar funcion para Escalar X y asginar el resultado a X_scaled
X_scaled = scaler.fit_transform(X)

########################################################
##FIN DE LA PARTE A COMPLETAR
########################################################






#Visualizamos el vector con los datos escalados
print(X_scaled[:2])

"""#### A continuacion realizaremos varias iteraciones con el algoritmo K-means, para poder visualizar con el metodo de elbow, cual es el mejor valor de K, para este conjunto de datos."""

# Instantiate the clustering model and visualizer
seed = 0

km = KMeans(init = 'k-means++'
            , max_iter=500
            , n_init=10
            , random_state=seed)


visualizer = KElbowVisualizer(km, k=(1,10))

visualizer.fit(X_scaled)        # Fit the data to the visualizer
visualizer.show()               # Finalize and render the figure

"""##### Aplicamos el metodo de ELBOW, y el resultado es que recomienda utilizar un valor de k=3, sin embargo visiblemente el codo se marca mejor con un k=2, ademas como ya sabemos de antemano que hay 2 tipos de tumores benignos y malignos, vamos a trabajar y considerar 2 grandes grupos, para mas adelante contrastar, si los grupos identificados por el algoritmo, estan alineados a los 2 tipos de tumores.


#### A continuacion vamos a aplicar el metodo de la silueta para entender que valor de K es mas conveniente. Esperemos que sean 2.
"""

nclusters = 2
seed = 0

model = KMeans(n_clusters=nclusters
            , init = 'k-means++'
            , max_iter=500
            , n_init=10
            , random_state=seed)


visualizer = SilhouetteVisualizer(model)

visualizer.fit(X_scaled)    # Fit the data to the visualizer
visualizer.show()    # Draw/show/show the data

nclusters = 3
seed = 0

model = KMeans(n_clusters=nclusters
            , init = 'k-means++'
            , max_iter=500
            , n_init=10
            , random_state=seed)


visualizer = SilhouetteVisualizer(model)

visualizer.fit(X_scaled)    # Fit the data to the visualizer
visualizer.show()    # Draw/show/show the data

"""##### Luego de aplicar el metodo de la SILUETA confirmamos que 2 clusteres, es lo adecuado. Confirmando el analisis visual del metodo de elbow.

##### A conitnuacion aplicaremos K-means, y luego una transformacion simple, ya que K-means, nomencla aleatoreamente los clusters detectados. Necesitamos forzar a que el CLUSTER MAYORITARIO el de mayor cantidad de elementos se le asigne el valor 0 y al minoritario 1. Para ir en linea, con el campo TARGET, ya que como confirmamos al inicio del analisis, identificamos que el grupo minoritario de tumores, son los del tipo maligno, por lo tanto le asignaremos el valor 1. Para que luego cuando utilicemos la matriz de confusion, se pueda considerar una coincidencia cuando ambos valores sean 1 o 0.
"""

########################################################
##INICIO DE LA PARTE A COMPLETAR
########################################################

# Crear el modelo de K-Means con 2 clusters (benigno y maligno) ademas tiene mejor puntaje de SILUETA
kmeans = KMeans(n_clusters=2, random_state=seed)

# Utilizar kMeans con la variables n_clusters=2 a "X_scaled" y alojar el resultado en "df['cluster']" utilizando el metodo fit_predict
df['cluster'] = kmeans.fit_predict(X_scaled)

########################################################
##FIN DE LA PARTE A COMPLETAR
########################################################





# Obtener el tamaño de cada cluster
tamano_clusters = df['cluster'].value_counts()

# Determinar el cluster mayoritario
cluster_mayoritario = tamano_clusters.idxmax()

# Asignar etiquetas basadas en el cluster mayoritario
df['cluster'] = df['cluster'].apply(lambda x: 0 if x == cluster_mayoritario else 1)


# Crear un gráfico de barras para la distribución de clusters
plt.figure(figsize=(6, 4))
ax = sns.countplot(x='cluster', data=df, palette='Set1')

# Agregar etiquetas con los números correspondientes a cada barra
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                textcoords='offset points')

plt.xlabel('Cluster')
plt.ylabel('Count')
plt.title('Distribution of Clusters')
plt.show()

# Crear una matriz de confusión para comparar con el campo "diagnosis"
conf_matrix = confusion_matrix(df['diagnosis'], df['cluster'])

# Mostrar la matriz de confusión como un mapa de calor
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#### La matriz de Confusion, presenta un MUY BUEN resultado, la mayoria de los casos estan bien clasificados, lo que indica que cada tipo de tumor, tiene caracteristicas muy distintivas que hacen que cada tipo de tumor sea muy identificable y pertenezca a un cluter especifico.

##### A continuacion aplicaremos PCA para poder visualizar en un grafico de 2 dimensiones los puntos, y poder pintar con colores diferentes cada cluster.
"""

########################################################
##INICIO DE LA PARTE A COMPLETAR
########################################################

# Aplicar PCA para reducir la dimensionalidad a 2 componentes, para mejorar el entrenamiento y visualizar clusters
pca = PCA(n_components=2)

# Utilizar la libreria PCA con (n_components=2), aplicarlo a "X_scaled" y alojar el resultado en "X_pca"
X_pca = pca.fit_transform(X_scaled)

########################################################
##FIN DE LA PARTE A COMPLETAR
########################################################



# Varianza explicada por cada componente
explained_variance = pca.explained_variance_ratio_
print("Varianza explicada por cada componente principal:", explained_variance)

"""##### Se puede observar que la varianza capturada con 2 componenentes principales es del 63% un valor bastante bajo, quiere decir que no representan fielmente a las originales, de todos modos vamos a visualizar los puntos. como valor agregado podrian utilizar un grafico 3D para lograr una mayor captura de varianza, y tener una visualizacion de los puntos mas confiable."""

from mpl_toolkits.mplot3d import Axes3D

# Aplicar PCA para reducir la dimensionalidad a 3 componentes
pca_3d = PCA(n_components=3)

# Utilizar la librería PCA con (n_components=3), aplicarlo a "X_scaled" y alojar el resultado en "X_pca_3d"
X_pca_3d = pca_3d.fit_transform(X_scaled)

# Visualizar los datos en un gráfico 3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Separar los puntos por cluster y graficarlos en diferentes colores
for cluster in range(2):
    ax.scatter(X_pca_3d[df['cluster'] == cluster, 0],
               X_pca_3d[df['cluster'] == cluster, 1],
               X_pca_3d[df['cluster'] == cluster, 2],
               label=f'Cluster {cluster}')

ax.set_xlabel('Componente Principal 1')
ax.set_ylabel('Componente Principal 2')
ax.set_zlabel('Componente Principal 3')
ax.set_title('Visualización PCA en 3D de los Clusters')
ax.legend()
plt.show()

# Varianza explicada por cada componente
explained_variance = pca_3d.explained_variance_ratio_
print("Varianza explicada por cada componente principal:", explained_variance)

# Crear un DataFrame con las nuevas variables PCA
df_pca = pd.DataFrame(data=X_pca, columns=['PCA1', 'PCA2'])

# Añadir la columna "diagnosis" al DataFrame PCA
df_pca['diagnosis'] = df['diagnosis']

# Visualizar el resultado en un scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PCA1', y='PCA2', hue='diagnosis', data=df_pca, palette='Set1')
plt.title('PCA - Breast Cancer Dataset')
plt.show()

"""##### A pesar del bajo grado de explicabilidad, se visualizan 2 clusters separados por una linea IMAGINARIA, no se detecta un espacio que los diferencie claramente."""

# Seleccionar las nuevas características PCA para clustering
X_pca_for_clustering = df_pca[['PCA1', 'PCA2']]

########################################################
##INICIO DE LA PARTE A COMPLETAR
########################################################

# Crear el modelo de K-Means con 2 clusters
# Utilizar kMeans con la variables n_clusters=2 a "X_pca_for_clustering" y alojar el resultado en "df_pca['cluster_pca']" utilizando el metodo fit_predict
# La idea es aplicar nuevamente K-means como antes, pero en vez utilizarlo en "X_scaled", lo aplicaremos directamente en "X_pca_for_clustering", que tiene solo
# 2 componentes luego de aplicar PCA, buscando mejorar los resultados y tiempos de ejecucion.

kmeans_pca = KMeans(n_clusters=2, random_state=seed)
df_pca['cluster_pca'] = kmeans_pca.fit_predict(X_pca_for_clustering)

########################################################
##FIN DE LA PARTE A COMPLETAR
########################################################





# Obtener el tamaño de cada cluster
tamano_clusters = df_pca['cluster_pca'].value_counts()

# Determinar el cluster mayoritario
cluster_mayoritario = tamano_clusters.idxmax()

# Asignar etiquetas basadas en el cluster mayoritario
df_pca['cluster_pca'] = df_pca['cluster_pca'].apply(lambda x: 0 if x == cluster_mayoritario else 1)

# Mostrar el resultado
print(df_pca['cluster_pca'].value_counts())

"""##### Aplicamos nuevamente clustering, pero ahora en vez de trabajar con todo el dataset escalado, direcamente utilizamos las 2 componenentes principales identificadas, con esto queremos validar si obtenemos un mejor resultado, y en menor tiempo de ejecucion. Para validar si es verdad las ventajas que promete PCA, a la hora de entrenar algoritmos.  Como punto extra podria realizar mediciones del tiempo de entrenamiento de ambos algoritmos, entrenando un modelo k-means con 2 componentes principales vs trabajar con las variables originales que son muchas mas."""

# Visualizar el resultado en un scatter plot con los clusters de K-Means
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PCA1', y='PCA2', hue='cluster_pca', data=df_pca, palette='Set1', legend='full')
plt.title('K-Means Clustering after PCA')
plt.show()

df_pca.sample(20)

# Crear una matriz de confusión para comparar los clusters con el campo "diagnosis"
conf_matrix_pca = confusion_matrix(df_pca['diagnosis'], df_pca['cluster_pca'])

# Mostrar la matriz de confusión como un mapa de calor
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_pca, annot=True, fmt='g', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix after PCA and K-Means Clustering')
plt.show()

"""##### Cuando analizamos la precision con PCA, logramos una MEJORA MINIMA, practicamente es el mismo resultado, en este caso, no logramos una mejora sustancia en el resultado, pero seguramente si se tratara de un volumen muy grande de datos, seria mejor entrenar el modelo con las nuevas variables, ya que el algoritmo logra practicamente el mismo resultado.  Por otro lado utilizar 3 componentes podria generar alcanzar un mejor resultado, ya que confirmamos que las 2 componentes principales, no representan con gran fiabilidad a las originales."""

# Crear un scatter plot
plt.figure(figsize=(8, 6))

# Asignar colores según la lógica dada
colors = df_pca.apply(lambda row: 'OK' if row['diagnosis'] == row['cluster_pca'] else 'NoK', axis=1)

sns.scatterplot(x='PCA1', y='PCA2', data=df_pca, hue=colors, palette=['blue', 'red'], s=50)
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.title('Scatter Plot with Color-Coded Diagnosis and Cluster')
plt.legend(title='Matching')
plt.show()

"""##### En este grafico marcamos en ROJO los casos mal clasificados, y realmente es algo esperable, ya que son casos que se encuentran en la division de los 2 clusters, por lo tanto, existian grandes posibilidades de que sean mal clasificados, un punto importante aqui, seria evaluar otro metodo de reduccion de dimensionalidad, que separe mas los puntos o evaluar otro algoritmo de clustering para confirmar si se arriba a mejores resultados.  Tambien se podria evaluar trabajar con mas componentes principales, ya que la representatividad de solo 2 componentes es baja. Aqui se compara el TARGET orginal vs Cluster identificado.

##### De todos modos considero, que el algoritmo de CLUSTERING logro un RESULTADO EXCELENTE, clasificando la mayoria de los casos de forma correcta de los 569 casos, 516 fueron clasificados correctamente. Logrando una precision del 90.6%

##### Lo mas interesante de esto, es que el problema encontro una solucion utilizando un algoritmo de CLUSTERING, y seria util en el caso de datos NO ETIQUETADOS. Aqui solo usamos las etiquetas, para que se pueda realizar una comparacion entre el resultado obtenido, y una clasificacion real.

"""

# Aplicar TSNE para reducir la dimensionalidad a 2 componentes, para mejorar el entrenamiento y visualizar clusters

from sklearn.manifold import TSNE



########################################################
##INICIO DE LA PARTE A COMPLETAR
########################################################

# Aplicar t-SNE para reducir la dimensionalidad a 2 componentes, para mejorar el entrenamiento y visualizar clusters, similar a PCA, pero ahora aplicamos otro algoritmo
tsne = TSNE(n_components=2, random_state=seed)

# Utilizar la libreria TSNE con (n_components=2), aplicarlo a "X_scaled" y alojar el resultado en "X_tsne"
X_tsne = tsne.fit_transform(X_scaled)

########################################################
##FIN DE LA PARTE A COMPLETAR
########################################################

# Crear un DataFrame con las nuevas variables t-SNE
df_tsne = pd.DataFrame(data=X_tsne, columns=['t-SNE1', 't-SNE2'])

# Añadir la columna "diagnosis" al DataFrame t-SNE
df_tsne['diagnosis'] = df['diagnosis']

# Visualizar el resultado en un scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='t-SNE1', y='t-SNE2', hue='diagnosis', data=df_tsne, palette='Set1')
plt.title('t-SNE - Breast Cancer Dataset')
plt.show()

# Seleccionar las nuevas características t-SNE para clustering
X_tsne_for_clustering = df_tsne[['t-SNE1', 't-SNE2']]

# Crear el modelo de K-Means con 2 clusters
kmeans_tsne = KMeans(n_clusters=2, random_state=42)
df_tsne['cluster_tsne'] = kmeans_tsne.fit_predict(X_tsne_for_clustering)

# Obtener el tamaño de cada cluster
tamano_clusters_tsne = df_tsne['cluster_tsne'].value_counts()

# Determinar el cluster mayoritario
cluster_mayoritario_tsne = tamano_clusters_tsne.idxmax()

# Asignar etiquetas basadas en el cluster mayoritario
df_tsne['cluster_tsne'] = df_tsne['cluster_tsne'].apply(lambda x: 0 if x == cluster_mayoritario_tsne else 1)


# Crear una matriz de confusión para comparar los clusters con el campo "diagnosis"
conf_matrix_tsne = confusion_matrix(df_tsne['diagnosis'], df_tsne['cluster_tsne'])

# Mostrar la matriz de confusión como un mapa de calor
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_tsne, annot=True, fmt='g', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix after t-SNE and K-Means Clustering')
plt.show()

"""#### A Continuacion debe:

##### Incluir sus CONCLUSIONES, sobre el analisis y el desarrollo aqui expuesto (Necesario)
##### Profundizar aplicando otros algoritmos de CLUSTERING y reduccion de dimensionalidad, para intentar encontrar un mejor resultado (Opcional)
"""

########################################################
##INICIO DE LA PARTE A COMPLETAR
########################################################


##### Incluir sus CONCLUSIONES, sobre el analisis y el desarrollo aqui expuesto (Necesario)
##### Profundizar aplicando otros algoritmos de CLUSTERING y reduccion de dimensionalidad, para intentar encontrar un mejor resultado (Opcional)


########################################################
##FIN DE LA PARTE A COMPLETAR
########################################################

#Agrupamiento jerarquico
from sklearn.cluster import AgglomerativeClustering

pca_agg = PCA(n_components=2)
X_pca_agg = pca.fit_transform(X_scaled)

agg_clustering = AgglomerativeClustering(n_clusters=2)
agg_labels = agg_clustering.fit_predict(X_scaled)

# Visualización de los clusters obtenidos
plt.figure(figsize=(8, 6))
plt.scatter(X_pca_agg[:, 0], X_pca_agg[:, 1], c=agg_labels, cmap='viridis')
plt.title('Agrupamiento Jerárquico con PCA')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.colorbar(label='Cluster')
plt.show()

#DBSCAN

from sklearn.cluster import DBSCAN
from sklearn.model_selection import GridSearchCV

# Definir los parámetros a buscar
param_grid = {'eps': [0.1, 0.5, 1.0],
              'min_samples': [5, 10, 15]}

# Crear una instancia de DBSCAN
dbscan = DBSCAN()

# Realizar la búsqueda de los mejores parámetros utilizando GridSearchCV
grid_search = GridSearchCV(dbscan, param_grid, cv=5, scoring='adjusted_rand_score')
grid_search.fit(X)

# Obtener el mejor modelo y sus parámetros
best_dbscan = grid_search.best_estimator_
best_params = grid_search.best_params_

# Predecir los clusters
df['cluster_db'] = best_dbscan.fit_predict(X)

# Imprimir los mejores parámetros
print("Mejores parámetros:", best_params)

"""# Conclusiones

La primera parte antes de empezar a entrenar el algoritmo utilizando las diversas técnicas de clustering es aplicar un escalador ajusta sus parámetros según los datos proporcionados y una vez ajustado, el escalador aplica la transformación a los datos. La razón principal para combinar el ajuste y la transformación en un solo paso es para simplificar el proceso y evitar que algunos datos sean tomados con mayor importancia que los demás.

En cuanto a la visualización de los clusters, esta te permite interpretar y validar los resultados del algoritmo de clustering. Puedes identificar la estructura y la distribución de los datos en los clusters para confirmar si son coherentes y significativos desde un punto de vista práctico. Los valores de silueta cercanos a +1 indican que los puntos están bien agrupados, mientras que valores cercanos a -1 indican que los puntos están mal agrupados, mientras que el método del codo es una técnica utilizada para determinar el número óptimo de clusters en un conjunto de datos.

Cuando se entrenó KMeans, este arrojo buenos resultado identificando cada variable con respecto a su target. Esto significa que hay 339 instancias que pertenecen a la clase 0 y fueron correctamente clasificadas como clase 0, dejando 18 falsos positivos hay 18 instancias que pertenecen a la clase 0 pero fueron incorrectamente clasificadas como clase 1. 36 Falsos negativos 36 instancias que pertenecen a la clase 1 pero fueron incorrectamente clasificadas como clase 0. Y el resto 176 como verdaderos positivos.

Al intentar visualizar la dimensionalidad de PCA en 3 componentes, entiendo que el aporte no fue signiticativo en relación a dejarlo en solo dos ya que solo resulto en una varianza de  0.09393163.

Luego de aplicar esto de reducir la dimensionalidad se ha podido ver mejoras en la predicción de la matriz de confusión.

Con el método de agrupamiento jerarquico, se han obtenido valores similares que los anteriores. Tambien he tratado de utilizar DBSCAN y encontrar los mejores parámetros para obtener mejores resultados al entrenar el modelo, lo cual puede ser bastante beneficioso para esta tipo de clasificación.

"""